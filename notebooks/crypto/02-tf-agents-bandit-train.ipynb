{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "\n",
    "try:\n",
    "  import tf_agents\n",
    "except ImportError:\n",
    "  %pip install tf-agents\n",
    "  %pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpus: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"gpus: {gpus}\")\n",
    "\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data: (111560, 15)\n",
      "observation training data: (17849, 30)\n",
      "price training data: (17849, 5)\n",
      "observation testing data: (4463, 30)\n",
      "price testing data: (4463, 5)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "from trading_utils import preprocess_data, create_wide_format_data, SYMBOLS\n",
    "from trading_env import CryptoTradingEnvironment\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Data and Model Paths\n",
    "DATA_FILEPATH = 'data/ohlcv.csv.gz'\n",
    "POLICY_SAVE_PATH = 'policy'\n",
    "\n",
    "# Model Hyperparameters\n",
    "NUM_TRAINING_STEPS = 1000  # Increased for more meaningful training\n",
    "NUM_TESTING_STEPS = 500\n",
    "ALPHA = 1.0 # LinUCB exploration parameter\n",
    "TEST_SIZE = 0.2\n",
    "SEED_FUND = 100.0\n",
    "TRADE_SIZE = 20.0\n",
    "TRADE_FEE = 0.00025\n",
    "INVALID_ACTION_PENALTY = -0.00005\n",
    "\n",
    "TECHNICAL_FEATURES = ['price_change', 'volume_change', 'rsi', 'macd', 'macd_signal', 'macd_hist', 'bb_percent_b', 'bb_bandwidth']\n",
    "TECHNICAL_FEATURES = ['price_change', 'rsi', 'macd_signal', 'macd_hist', 'bb_percent_b', 'bb_bandwidth']\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(DATA_FILEPATH, compression='gzip', parse_dates=['timestamp']) #.set_index('timestamp')\n",
    "all_data = preprocess_data(df)#.iloc[2000:] # TODO: remove this\n",
    "observation_df, prices_df = create_wide_format_data(\n",
    "    all_data, \n",
    "    symbols=SYMBOLS, \n",
    "    features=TECHNICAL_FEATURES\n",
    ")\n",
    "\n",
    "train_observation_df, test_observation_df = train_test_split(\n",
    "    observation_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_prices_df, test_prices_df = train_test_split(\n",
    "    prices_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_observation_scaled_df = pd.DataFrame(scaler.fit_transform(train_observation_df), columns=train_observation_df.columns, index=train_observation_df.index)\n",
    "test_observation_scaled_df = pd.DataFrame(scaler.transform(test_observation_df), columns=train_observation_df.columns, index=test_observation_df.index)\n",
    "\n",
    "\n",
    "print(f\"all_data: {all_data.shape}\")\n",
    "print(f\"observation training data: {train_observation_scaled_df.shape}\")\n",
    "print(f\"price training data: {train_prices_df.shape}\")\n",
    "print(f\"observation testing data: {test_observation_scaled_df.shape}\")\n",
    "print(f\"price testing data: {test_prices_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 1000 steps...\n",
      "Resetting the environment, including the portfolio.\n",
      "Resetting the environment.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'trade_history_amount' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 83\u001b[0m\n\u001b[1;32m     75\u001b[0m driver \u001b[38;5;241m=\u001b[39m dynamic_step_driver\u001b[38;5;241m.\u001b[39mDynamicStepDriver(\n\u001b[1;32m     76\u001b[0m     env\u001b[38;5;241m=\u001b[39mtf_env,\n\u001b[1;32m     77\u001b[0m     policy\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39mpolicy,\n\u001b[1;32m     78\u001b[0m     num_steps\u001b[38;5;241m=\u001b[39mnum_steps_to_train,\n\u001b[1;32m     79\u001b[0m     observers\u001b[38;5;241m=\u001b[39m[train_step, metrics_observer, ShowProgress(num_steps_to_train)]\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_steps_to_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Save Policy\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/drivers/dynamic_step_driver.py:193\u001b[0m, in \u001b[0;36mDynamicStepDriver.run\u001b[0;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, time_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, policy_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Takes steps in the environment using the policy while updating observers.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    policy_state: Tensor with final step policy state.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/utils/common.py:193\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m check_tf1_allowed()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    192\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/drivers/dynamic_step_driver.py:215\u001b[0m, in \u001b[0;36mDynamicStepDriver._run\u001b[0;34m(self, time_step, policy_state, maximum_iterations)\u001b[0m\n\u001b[1;32m    208\u001b[0m batch_dims \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39mget_outer_shape(\n\u001b[1;32m    209\u001b[0m     time_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mtime_step_spec()\n\u001b[1;32m    210\u001b[0m )\n\u001b[1;32m    211\u001b[0m counter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros(batch_dims, tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    213\u001b[0m [_, time_step, policy_state] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    214\u001b[0m     tf\u001b[38;5;241m.\u001b[39mstop_gradient,\n\u001b[0;32m--> 215\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop_condition_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop_body_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcounter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdriver_loop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    223\u001b[0m )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time_step, policy_state\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/deprecation.py:660\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    653\u001b[0m         _log_deprecation(\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    659\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/while_loop.py:241\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/while_loop.py:488\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    485\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m    486\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m--> 488\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    490\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_agents/drivers/dynamic_step_driver.py:151\u001b[0m, in \u001b[0;36mDynamicStepDriver._loop_body_fn.<locals>.loop_body\u001b[0;34m(counter, time_step, policy_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m observer_ops \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m observer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observers:\n\u001b[0;32m--> 151\u001b[0m   \u001b[43mobserver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Latest op in graph is the call op for above fn so get it.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     observer_ops\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    155\u001b[0m         tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39mget_operations()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    156\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mMetricsObserver.__call__\u001b[0;34m(self, trajectory)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trajectory\u001b[38;5;241m.\u001b[39mis_boundary():\n\u001b[1;32m     43\u001b[0m     reward \u001b[38;5;241m=\u001b[39m trajectory\u001b[38;5;241m.\u001b[39mreward\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m     optimal_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oracle_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     regret \u001b[38;5;241m=\u001b[39m optimal_reward \u001b[38;5;241m-\u001b[39m reward\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m/tf/notebooks/crypto/trading_env.py:368\u001b[0m, in \u001b[0;36mCryptoTradingEnvironment.optimal_reward_oracle\u001b[0;34m(self, trajectory)\u001b[0m\n\u001b[1;32m    365\u001b[0m         reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalid_action_penalty\n\u001b[1;32m    367\u001b[0m     possible_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m--> 368\u001b[0m     possible_trades\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrade_history_amount\u001b[49m)\n\u001b[1;32m    370\u001b[0m optimal_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(possible_rewards)\n\u001b[1;32m    371\u001b[0m optimal_symbol, optimal_trade_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_action(optimal_action)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'trade_history_amount' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Setup Environment\n",
    "crypto_env = CryptoTradingEnvironment(\n",
    "    observation_df=train_observation_scaled_df,\n",
    "    prices_df=train_prices_df,\n",
    "    symbols=SYMBOLS,\n",
    "    seed_fund=SEED_FUND,\n",
    "    trade_size=TRADE_SIZE,\n",
    "    trade_fee=TRADE_FEE,\n",
    "    invalid_action_penalty=INVALID_ACTION_PENALTY,\n",
    ")\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(crypto_env)\n",
    "\n",
    "# Setup Agent\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec(),\n",
    "    alpha=ALPHA,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "class ShowProgress:\n",
    "    def __init__(self, total, interval=50):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "        self.interval = interval\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % self.interval == 0:\n",
    "            reward = np.round(trajectory.reward.numpy()[0], 5)\n",
    "            print(f\"\\r{self.counter}/{self.total} Reward: {reward}\", end=\"\")\n",
    "\n",
    "class MetricsObserver:\n",
    "    def __init__(self, oracle_fn):\n",
    "        self._oracle_fn = oracle_fn\n",
    "        self.rewards = []\n",
    "        self.regrets = []\n",
    "        self.optimal_rewards = []\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            reward = trajectory.reward.numpy()[0]\n",
    "            optimal_reward = self._oracle_fn(trajectory)\n",
    "            regret = optimal_reward - reward\n",
    "            \n",
    "            self.rewards.append(reward)\n",
    "            self.regrets.append(regret)\n",
    "            self.optimal_rewards.append(optimal_reward)\n",
    "\n",
    "    def cum_rewards(self):\n",
    "        cumulative_reward = np.cumsum(self.rewards)\n",
    "        cumulative_optimal_reward = np.cumsum(self.optimal_rewards)\n",
    "        return cumulative_reward, cumulative_optimal_reward\n",
    "\n",
    "    def cum_averages(self):\n",
    "        steps = np.arange(len(self.rewards)) + 1\n",
    "        cumulative_avg_reward = np.cumsum(self.rewards) / steps\n",
    "        cumulative_avg_optimal_reward = np.cumsum(self.optimal_rewards) / steps\n",
    "        cumulative_avg_regret = np.cumsum(self.regrets) / steps\n",
    "        return cumulative_avg_reward, cumulative_avg_optimal_reward, cumulative_avg_regret\n",
    "        \n",
    "\n",
    "def train_step(trajectory):\n",
    "    if not trajectory.is_last():\n",
    "        time_axised_trajectory = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), trajectory)\n",
    "        agent.train(time_axised_trajectory)\n",
    "        \n",
    "metrics_observer = MetricsObserver(tf_env.pyenv.envs[0].optimal_reward_oracle)\n",
    "\n",
    "# 5. Setup Driver\n",
    "# Set num_steps to the total number of steps you want to train for.\n",
    "num_steps_to_train = min(len(train_observation_scaled_df) - 1, NUM_TRAINING_STEPS)\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=num_steps_to_train,\n",
    "    observers=[train_step, metrics_observer, ShowProgress(num_steps_to_train)]\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting training for {num_steps_to_train} steps...\")\n",
    "driver.run()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save Policy\n",
    "print(f\"\\nSaving the trained policy to: {POLICY_SAVE_PATH}\")\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "saver.save(POLICY_SAVE_PATH)\n",
    "print(\"Policy saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_avg_reward, cumulative_avg_optimal_reward, cumulative_avg_regret = metrics_observer.cum_averages()\n",
    "\n",
    "plt.plot(cumulative_avg_regret)\n",
    "plt.title('Cumulative Average Regret')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_avg_reward, label='Actual')\n",
    "plt.plot(cumulative_avg_optimal_reward, label='Optimal')\n",
    "plt.title('Actual vs. Optimal Cumulative Average Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_reward, cumulative_optimal_reward = metrics_observer.cum_rewards()\n",
    "\n",
    "plt.plot(cumulative_reward, label='Actual')\n",
    "plt.plot(cumulative_optimal_reward, label='Optimal')\n",
    "plt.title('Actual vs. Optimal Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_history = tf_env.pyenv.envs[0].balance_history\n",
    "asset_value_history = tf_env.pyenv.envs[0].asset_value_history\n",
    "portfolio_value_history = tf_env.pyenv.envs[0].portfolio_value_history\n",
    "\n",
    "# Plot the portfolio value over time\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.stackplot(train_prices_df.index[:len(balance_history)],[balance_history, asset_value_history], labels=['Cash Balance','Asset Value'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Portfolio Value Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history = tf_env.pyenv.envs[0].trade_history\n",
    "\n",
    "trade_history_df = pd.DataFrame(trade_history, columns=['Symbol', 'Valid', 'Type', 'Value'])\n",
    "trade_history_df['Type'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history_df['Valid'].value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_to_test = min(len(test_observation_scaled_df) - 1, NUM_TESTING_STEPS)\n",
    "\n",
    "eval_env = CryptoTradingEnvironment(\n",
    "    observation_df=test_observation_scaled_df[:num_steps_to_test],\n",
    "    prices_df=test_prices_df[:num_steps_to_test],\n",
    "    symbols=SYMBOLS,\n",
    "    seed_fund=SEED_FUND,\n",
    "    trade_size=TRADE_SIZE,\n",
    "    trade_fee=TRADE_FEE,\n",
    "    invalid_action_penalty=INVALID_ACTION_PENALTY,\n",
    ")\n",
    "\n",
    "# 4. Run the Evaluation Loop\n",
    "print(f\"\\n--- Starting Evaluation on Test Set ({len(test_observation_scaled_df)} steps) ---\")\n",
    "\n",
    "time_step = eval_env.reset()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Loop until the environment signals the episode is over\n",
    "while not time_step.is_last():\n",
    "    # The policy expects a batch dimension. We create a new TimeStep\n",
    "    # object with the observation correctly batched.\n",
    "    counter += 1\n",
    "    if counter % 50 == 0:\n",
    "        print(f\"\\rTraining step {counter}/{num_steps_to_test}\", end=\"\")\n",
    "    \n",
    "    # A. Add the batch dimension to the observation tensor.\n",
    "    observation_batch = tf.expand_dims(time_step.observation, 0)\n",
    "    \n",
    "    # B. Use the ._replace() method to create a new TimeStep with the batched observation.\n",
    "    # This is the correct way to modify a TimeStep.\n",
    "    batched_time_step = time_step._replace(observation=observation_batch)\n",
    "    \n",
    "    # C. Get an action from the loaded policy using the batched TimeStep.\n",
    "    action_step = agent.policy.action(batched_time_step)\n",
    "    action = action_step.action.numpy()[0]\n",
    "    \n",
    "    # D. Step the environment with the original, unbatched action.\n",
    "    time_step = eval_env.step(action)\n",
    "\n",
    "print(\"\\n--- Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Report and Plot the Results\n",
    "balance_history = eval_env.balance_history\n",
    "asset_value_history = eval_env.asset_value_history\n",
    "portfolio_value_history = eval_env.portfolio_value_history\n",
    "seed_fund_value = balance_history[0]\n",
    "final_portfolio_value = portfolio_value_history[-1]\n",
    "total_return_pct = ((final_portfolio_value - seed_fund_value) / seed_fund_value) * 100\n",
    "\n",
    "print(f\"Initial Portfolio Value: ${seed_fund_value:,.2f}\")\n",
    "print(f\"Final Portfolio Value:   ${final_portfolio_value:,.2f}\")\n",
    "print(f\"Total Return on Test Set: {total_return_pct:.2f}%\")\n",
    "\n",
    "# Plot the portfolio value over time\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.stackplot(test_prices_df.index[:len(balance_history)],[balance_history, asset_value_history], labels=['Cash Balance','Asset Value'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Agent Performance on Hold-Out Test Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
