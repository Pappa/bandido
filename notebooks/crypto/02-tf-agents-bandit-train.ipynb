{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore training data\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Pappa/bandido/blob/main/notebooks/crypto/02-tf-agents-bandit-train.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import tf_agents\n",
    "except ImportError:\n",
    "  %pip install tf-agents\n",
    "  %pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "from trading_utils import preprocess_data, create_wide_format_data, SYMBOLS\n",
    "from trading_env import CryptoTradingEnvironment\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Data and Model Paths\n",
    "DATA_FILEPATH = 'data/ohlcv.csv.gz'\n",
    "POLICY_SAVE_PATH = 'policy'\n",
    "\n",
    "# Model Hyperparameters\n",
    "CONTEXT_LENGTH = 10\n",
    "NUM_TRAINING_STEPS = 1000  # Increased for more meaningful training\n",
    "ALPHA = 1.0 # LinUCB exploration parameter\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "TECHNICAL_FEATURES = ['rsi', 'macd', 'macd_signal', 'macd_hist', 'bb_percent_b', 'bb_bandwidth']\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(DATA_FILEPATH, compression='gzip', parse_dates=['timestamp']) #.set_index('timestamp')\n",
    "all_data = preprocess_data(df)\n",
    "observation_df, prices_df = create_wide_format_data(\n",
    "    all_data, \n",
    "    symbols=SYMBOLS, \n",
    "    features=TECHNICAL_FEATURES\n",
    ")\n",
    "\n",
    "print(f\"all_data: {len(all_data)}\")\n",
    "\n",
    "train_observation_df, test_observation_df = train_test_split(\n",
    "    observation_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_prices_df, test_prices_df = train_test_split(\n",
    "    prices_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_observation_scaled_df = pd.DataFrame(scaler.fit_transform(train_observation_df), columns=train_observation_df.columns)\n",
    "test_observation_scaled_df = pd.DataFrame(scaler.transform(test_observation_df), columns=train_observation_df.columns)\n",
    "\n",
    "# 2. Setup Environment\n",
    "crypto_env = CryptoTradingEnvironment(\n",
    "    observation_df=train_observation_scaled_df,\n",
    "    prices_df=train_prices_df,\n",
    "    symbols=SYMBOLS\n",
    ")\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(crypto_env)\n",
    "\n",
    "# 3. Setup Agent\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec(),\n",
    "    alpha=ALPHA,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# 4. Setup Metrics and Oracle\n",
    "def optimal_reward_oracle(observation: np.ndarray) -> np.float32:\n",
    "    \"\"\"\n",
    "    Calculates the best possible reward for the current step by looking ahead.\n",
    "    This \"perfect foresight\" oracle is used for calculating regret.\n",
    "\n",
    "    NOTE: The 'observation' argument is unused but required by the metric's API.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. Get the current step index from the environment's state.\n",
    "    current_step = crypto_env.current_step\n",
    "    price_data = crypto_env.price_data\n",
    "    \n",
    "    # 3. Handle the edge case where we are at the end of the data.\n",
    "    # We can't look one step into the future.\n",
    "    if current_step >= len(price_data) - 1:\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Calculate the reward for every possible action to find the maximum.\n",
    "    all_possible_rewards = []\n",
    "    num_actions = crypto_env.action_spec().maximum + 1\n",
    "    \n",
    "    for action in range(num_actions):\n",
    "        # Decode the action into a symbol and a trade type\n",
    "        crypto_index = action // 3\n",
    "        trade_type_idx = action % 3  # 0: BUY, 1: HOLD, 2: SELL\n",
    "        \n",
    "        # The reward for a HOLD action is always 0.\n",
    "        if trade_type_idx == 1:\n",
    "            all_possible_rewards.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        symbol_to_trade = crypto_env.symbols[crypto_index]\n",
    "        \n",
    "        current_price = price_data.iloc[current_step][symbol_to_trade]\n",
    "        next_price = price_data.iloc[current_step + 1][symbol_to_trade]\n",
    "        # next_price_change = price_data.iloc[current_step + 1][symbol_to_trade]\n",
    "\n",
    "        \n",
    "        # Calculate the reward for this specific BUY or SELL action\n",
    "        if trade_type_idx == 0:  # BUY\n",
    "            reward = (next_price - current_price) / current_price\n",
    "        else:  # SELL (trade_type_idx == 2)\n",
    "            reward = (current_price - next_price) / current_price\n",
    "        \n",
    "        all_possible_rewards.append(reward)\n",
    "        \n",
    "    # 5. Return the maximum possible reward from all actions.\n",
    "    return np.max(all_possible_rewards).astype(np.float32)\n",
    "\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_oracle)\n",
    "\n",
    "class ShowProgress:\n",
    "    def __init__(self, total, interval=50):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "        self.interval = interval\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % self.interval == 0:\n",
    "            print(\"\\r{}/{} Reward: {}\".format(self.counter, self.total, np.round(trajectory.reward.numpy()[0], 6)), end=\"\")\n",
    "\n",
    "class RewardCollector():\n",
    "    def __init__(self):\n",
    "        self._rewards = []\n",
    "    def __call__(self, trajectory):\n",
    "        self._rewards.append(trajectory.reward[0])\n",
    "    @property\n",
    "    def rewards(self):\n",
    "        return np.array(self._rewards)\n",
    "\n",
    "# 5. Setup Driver\n",
    "def train_step(trajectory):\n",
    "    if not trajectory.is_last():\n",
    "        time_axised_trajectory = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), trajectory)\n",
    "        agent.train(time_axised_trajectory)\n",
    "        \n",
    "reward_collector = RewardCollector()\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=len(SYMBOLS),\n",
    "    observers=[train_step, regret_metric, ShowProgress(NUM_TRAINING_STEPS), reward_collector]\n",
    ")\n",
    "\n",
    "# 6. Run Training\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.policy.trajectory_spec,\n",
    "    batch_size=len(SYMBOLS),\n",
    "    max_length=1)\n",
    "\n",
    "regret_values = []\n",
    "\n",
    "for _ in range(NUM_TRAINING_STEPS):\n",
    "  driver.run()\n",
    "  loss_info = agent.train(replay_buffer.gather_all())\n",
    "  replay_buffer.clear()\n",
    "  regret_values.append(regret_metric.result())\n",
    "\n",
    "\n",
    "# print(f\"\\nStarting training for {driver._num_steps} steps...\")\n",
    "# # driver.run()\n",
    "# print(\"\\nTraining finished.\")\n",
    "\n",
    "# 7. Save Policy\n",
    "print(f\"\\nSaving the trained policy to: {POLICY_SAVE_PATH}\")\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "saver.save(POLICY_SAVE_PATH)\n",
    "print(\"Policy saved successfully.\")\n",
    "\n",
    "# 8. Report Results\n",
    "cumulative_regret = regret_metric.result().numpy()\n",
    "print(regret_metric)\n",
    "print(f\"\\nCumulative Regret vs. Perfect Foresight Oracle: {cumulative_regret:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_regret = np.mean(regret_values)\n",
    "plt.axhline(y=average_regret, color='r', linestyle='-')\n",
    "plt.plot(regret_values)\n",
    "plt.ylabel('Average Regret')\n",
    "plt.xlabel('Number of Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = reward_collector.rewards\n",
    "average_reward = rewards.mean()\n",
    "\n",
    "plt.axhline(y=average_reward, color='r', linestyle='-')\n",
    "plt.plot(rewards)\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Number of Iterations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
