{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "from utils import preprocess_data, create_wide_format_data, SYMBOLS\n",
    "from environment import CryptoTradingEnvironment\n",
    "\n",
    "\n",
    "# Data and Model Paths\n",
    "DATA_FILEPATH = 'data/ohlcv.csv.gz'\n",
    "POLICY_SAVE_PATH = 'policy'\n",
    "\n",
    "# Model Hyperparameters\n",
    "CONTEXT_LENGTH = 10\n",
    "NUM_TRAINING_STEPS = 1000  # Increased for more meaningful training\n",
    "ALPHA = 1.0 # LinUCB exploration parameter\n",
    "\n",
    "# --- Main Training Script ---\n",
    "print(\"--- Starting Bandit Training Script ---\")\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(DATA_FILEPATH, compression='gzip', parse_dates=['timestamp']) #.set_index('timestamp')\n",
    "all_data = preprocess_data(df)\n",
    "observation_df, prices_df = create_wide_format_data(\n",
    "    all_data, \n",
    "    symbols=SYMBOLS, \n",
    "    features=['rsi']\n",
    ")\n",
    "training_data = all_data.iloc[:NUM_TRAINING_STEPS]\n",
    "\n",
    "# 2. Setup Environment\n",
    "tf_env = tf_py_environment.TFPyEnvironment(\n",
    "    CryptoTradingEnvironment(\n",
    "    observation_df=observation_df,\n",
    "    prices_df=prices_df, symbols=SYMBOLS)\n",
    ")\n",
    "\n",
    "# 3. Setup Agent\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec(),\n",
    "    alpha=ALPHA,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# 4. Setup Metrics and Oracle\n",
    "def optimal_reward_oracle(observation: np.ndarray) -> np.float32:\n",
    "    \"\"\"\n",
    "    Calculates the best possible reward for the current step by looking ahead.\n",
    "    This \"perfect foresight\" oracle is used for calculating regret.\n",
    "\n",
    "    It accesses the environment's internal state to get the current time step\n",
    "    and the pre-calculated prices DataFrame.\n",
    "\n",
    "    NOTE: The 'observation' argument is unused but required by the metric's API.\n",
    "    \"\"\"\n",
    "    # 1. Get a handle to the underlying Python environment.\n",
    "    py_env = tf_env.pyenv.envs[0]\n",
    "    \n",
    "    # 2. Get the current step index from the environment's state.\n",
    "    current_step = py_env.current_step\n",
    "    \n",
    "    # 3. Handle the edge case where we are at the end of the data.\n",
    "    # We can't look one step into the future.\n",
    "    if current_step >= len(py_env._price_data) - 1:\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Calculate the reward for every possible action to find the maximum.\n",
    "    all_possible_rewards = []\n",
    "    num_actions = py_env.action_spec().maximum + 1\n",
    "    \n",
    "    for action in range(num_actions):\n",
    "        # Decode the action into a symbol and a trade type\n",
    "        crypto_index = action // 3\n",
    "        trade_type_idx = action % 3  # 0: BUY, 1: HOLD, 2: SELL\n",
    "        \n",
    "        # The reward for a HOLD action is always 0.\n",
    "        if trade_type_idx == 1:\n",
    "            all_possible_rewards.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        symbol_to_trade = py_env.symbols[crypto_index]\n",
    "        \n",
    "        # PERFORMANCE WIN: Get prices from the fast, wide-format prices DataFrame.\n",
    "        current_price = py_env._price_data.iloc[current_step][symbol_to_trade]\n",
    "        next_price = py_env._price_data.iloc[current_step + 1][symbol_to_trade]\n",
    "        \n",
    "        # Calculate the reward for this specific BUY or SELL action\n",
    "        if trade_type_idx == 0:  # BUY\n",
    "            reward = (next_price - current_price) / current_price\n",
    "        else:  # SELL (trade_type_idx == 2)\n",
    "            reward = (current_price - next_price) / current_price\n",
    "        \n",
    "        all_possible_rewards.append(reward)\n",
    "        \n",
    "    # 5. Return the maximum possible reward from all actions.\n",
    "    return np.max(all_possible_rewards).astype(np.float32)\n",
    "\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_oracle)\n",
    "\n",
    "class ShowProgress:\n",
    "    def __init__(self, total, interval=50):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "        self.interval = interval\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % self.interval == 0:\n",
    "            print(\"\\r{}/{} Reward: {}\".format(self.counter, self.total, trajectory.reward), end=\"\")\n",
    "\n",
    "# 5. Setup Driver\n",
    "def train_step(trajectory):\n",
    "    if not trajectory.is_last():\n",
    "        time_axised_trajectory = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), trajectory)\n",
    "        agent.train(time_axised_trajectory)\n",
    "        \n",
    "num_steps = len(training_data) - CONTEXT_LENGTH - 5\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=num_steps,\n",
    "    observers=[train_step, regret_metric, ShowProgress(num_steps)]\n",
    ")\n",
    "\n",
    "# 6. Run Training\n",
    "print(f\"\\nStarting training for {driver._num_steps} steps...\")\n",
    "driver.run()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 7. Save Policy\n",
    "print(f\"\\nSaving the trained policy to: {POLICY_SAVE_PATH}\")\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "saver.save(POLICY_SAVE_PATH)\n",
    "print(\"Policy saved successfully.\")\n",
    "\n",
    "# 8. Report Results\n",
    "cumulative_regret = regret_metric.result().numpy()\n",
    "print(f\"\\nCumulative Regret vs. Perfect Foresight Oracle: {cumulative_regret:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.axhline(y=0.0, color='r', linestyle='-')\n",
    "# plt.plot(rewards)\n",
    "# plt.ylabel('Rewards')\n",
    "# plt.xlabel('Number of Iterations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
