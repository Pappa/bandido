{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"gpus: {gpus}\")\n",
    "\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "from utils import load_and_prepare_data, SYMBOLS\n",
    "\n",
    "# --- Environment Class ---\n",
    "class CryptoTradingEnvironment(py_environment.PyEnvironment):\n",
    "    def __init__(self, data, symbols, context_len=10):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._context_len = context_len\n",
    "        self._symbols = symbols\n",
    "        self._num_cryptos = len(self._symbols)\n",
    "        self._minimal_obs_size = self._num_cryptos * 2 \n",
    "        observation_size = self._minimal_obs_size * self._context_len\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=self._num_cryptos * 2 - 1, name='action'\n",
    "        )\n",
    "        self._observation_spec = array_spec.ArraySpec(\n",
    "            shape=(observation_size,), dtype=np.float32, name='context'\n",
    "        )\n",
    "        data_spec = tensor_spec.TensorSpec([self._minimal_obs_size], dtype=tf.float32, name='minimal_observation')\n",
    "        self._replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            data_spec=data_spec, batch_size=1, max_length=self._context_len + 5\n",
    "        )\n",
    "        self._current_step_index = 0\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _get_minimal_observation(self, index):\n",
    "        obs_slice = []\n",
    "        for symbol in self._symbols:\n",
    "            obs_slice.append(self._data.iloc[index][f'{symbol}_close_return'])\n",
    "            obs_slice.append(self._data.iloc[index][f'{symbol}_volume_return'])\n",
    "        return np.array(obs_slice, dtype=np.float32)\n",
    "\n",
    "    def _observe(self):\n",
    "        num_items_in_buffer = self._replay_buffer.num_frames()\n",
    "        if num_items_in_buffer == 0:\n",
    "            return np.zeros(self._observation_spec.shape, dtype=np.float32)\n",
    "\n",
    "        dataset = self._replay_buffer.as_dataset(single_deterministic_pass=True)\n",
    "        batched_items = next(iter(dataset.batch(num_items_in_buffer)))\n",
    "        \n",
    "        # Select only the data tensor (at index 0) from the (data, info) tuple\n",
    "        all_items = batched_items[0]\n",
    "        context = tf.reshape(all_items[-self._context_len:], [-1])\n",
    "        return context.numpy()\n",
    "\n",
    "    def _reset(self):\n",
    "        self._replay_buffer.clear()\n",
    "        self._episode_ended = False\n",
    "        self._current_step_index = self._context_len \n",
    "        for i in range(self._current_step_index):\n",
    "            self._replay_buffer.add_batch(tf.expand_dims(self._get_minimal_observation(i), 0))\n",
    "        return ts.restart(self._observe())\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "        crypto_index = action // 2\n",
    "        is_buy_action = action % 2 == 0\n",
    "        column_name = f'{self._symbols[crypto_index]}_close'\n",
    "        current_price = self._data.iloc[self._current_step_index][column_name]\n",
    "        next_price = self._data.iloc[self._current_step_index + 1][column_name]\n",
    "        reward = ((next_price - current_price) / current_price) if is_buy_action else ((current_price - next_price) / current_price)\n",
    "        self._replay_buffer.add_batch(tf.expand_dims(self._get_minimal_observation(self._current_step_index), 0))\n",
    "        self._current_step_index += 1\n",
    "        if self._current_step_index >= len(self._data) - 2:\n",
    "            self._episode_ended = True\n",
    "        observation = self._observe()\n",
    "        return ts.termination(observation, reward) if self._episode_ended else ts.transition(observation, reward)\n",
    "\n",
    "# --- Configuration & Main Script ---\n",
    "CRYPTO_NAMES = SYMBOLS\n",
    "DATA_FILEPATH = 'data/ohlcv.csv.gz'\n",
    "CONTEXT_LENGTH = 10\n",
    "NUM_TRAINING_STEPS = 1000\n",
    "POLICY_SAVE_PATH = 'policy' # Directory to save the policy\n",
    "\n",
    "\n",
    "data = load_and_prepare_data(DATA_FILEPATH, CRYPTO_NAMES)\n",
    "\n",
    "if NUM_TRAINING_STEPS > len(data) - CONTEXT_LENGTH - 5:\n",
    "    NUM_TRAINING_STEPS = len(data) - CONTEXT_LENGTH - 5\n",
    "    print(f\"\\nWarning: Training steps reduced to {NUM_TRAINING_STEPS} to fit available data.\")\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(CryptoTradingEnvironment(data, symbols=CRYPTO_NAMES, context_len=CONTEXT_LENGTH))\n",
    "\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec(),\n",
    "    alpha=1.0,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "def train_step(trajectory):        \n",
    "    if not trajectory.is_last():\n",
    "        time_axised_trajectory = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), trajectory)\n",
    "        agent.train(time_axised_trajectory)\n",
    "\n",
    "def optimal_reward_oracle(observation):\n",
    "    \"\"\"\n",
    "    Calculates the best possible reward for the current step by looking ahead.\n",
    "    NOTE: The 'observation' is unused, but required by the metric's API.\n",
    "    We rely on the environment's internal state.\n",
    "    \"\"\"\n",
    "    # Get the python environment to access its internal state\n",
    "    py_env = tf_env.pyenv.envs[0]\n",
    "    current_step = py_env._current_step_index\n",
    "    \n",
    "    if current_step >= len(py_env._data) - 2:\n",
    "        return 0.0 # No future data available\n",
    "\n",
    "    all_possible_rewards = []\n",
    "    num_actions = py_env.action_spec().maximum + 1\n",
    "    \n",
    "    for action in range(num_actions):\n",
    "        crypto_index = action // 2\n",
    "        is_buy_action = action % 2 == 0\n",
    "        symbol = py_env._symbols[crypto_index]\n",
    "        column_name = f'{symbol}_close'\n",
    "        \n",
    "        current_price = py_env._data.iloc[current_step][column_name]\n",
    "        next_price = py_env._data.iloc[current_step + 1][column_name]\n",
    "        \n",
    "        reward = ((next_price - current_price) / current_price) if is_buy_action else ((current_price - next_price) / current_price)\n",
    "        all_possible_rewards.append(reward)\n",
    "        \n",
    "    return np.max(all_possible_rewards).astype(np.float32)\n",
    "\n",
    "# Use our oracle with the RegretMetric\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_oracle)\n",
    "\n",
    "def get_action_name(action):\n",
    "    crypto_index = action // 2\n",
    "    action_type = \"BUY\" if action % 2 == 0 else \"SELL\"\n",
    "    return f\"{action_type} {CRYPTO_NAMES[crypto_index]}\"\n",
    "    \n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=NUM_TRAINING_STEPS,\n",
    "    observers=[train_step, regret_metric]\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting training for {NUM_TRAINING_STEPS} steps...\")\n",
    "driver.run()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- SAVING THE POLICY AFTER TRAINING ---\n",
    "print(f\"\\nSaving the trained policy to: {POLICY_SAVE_PATH}\")\n",
    "\n",
    "# 1. Create a PolicySaver instance for our agent's policy\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "\n",
    "# 2. Save the policy to the specified directory\n",
    "saver.save(POLICY_SAVE_PATH)\n",
    "\n",
    "print(\"Policy saved successfully.\")\n",
    "\n",
    "cumulative_regret = regret_metric.result().numpy()\n",
    "print(f\"cumulative_regret: {cumulative_regret}\")\n",
    "print(f\"\\nCumulative Regret vs. Perfect Foresight Oracle: {cumulative_regret:.4f}\")\n",
    "print(\"This measures the total profit the agent missed compared to a perfect model.\")\n",
    "\n",
    "print(\"\\n--- Evaluation Loop ---\")\n",
    "time_step = tf_env.reset()\n",
    "cumulative_reward = 0\n",
    "num_eval_steps = 100\n",
    "rewards = []\n",
    "for i in range(num_eval_steps):\n",
    "    if time_step.is_last():\n",
    "        print(\"Evaluation data ended. Resetting.\")\n",
    "        time_step = tf_env.reset()\n",
    "        \n",
    "    action_step = agent.policy.action(time_step)\n",
    "    action = action_step.action.numpy()[0]\n",
    "    time_step = tf_env.step(action)\n",
    "    reward = time_step.reward.numpy()[0]\n",
    "    cumulative_reward += reward\n",
    "    rewards.append(reward)\n",
    "\n",
    "print(f\"\\nFinal cumulative reward over last {num_eval_steps} steps: {cumulative_reward:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.axhline(y=0.0, color='r', linestyle='-')\n",
    "plt.plot(rewards)\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Number of Iterations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
