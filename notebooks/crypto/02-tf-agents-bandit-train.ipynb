{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 17:51:27.714140: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-20 17:51:27.741774: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-20 17:51:27.741793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-20 17:51:27.742719: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-20 17:51:27.747651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import tf_agents\n",
    "except ImportError:\n",
    "  %pip install tf-agents\n",
    "  %pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data: (111560, 15)\n",
      "observation training data: (17849, 30)\n",
      "price training data: (17849, 5)\n",
      "observation testing data: (4463, 30)\n",
      "price testing data: (4463, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "from trading_utils import preprocess_data, create_wide_format_data, SYMBOLS\n",
    "from trading_env import CryptoTradingEnvironment\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Data and Model Paths\n",
    "DATA_FILEPATH = 'data/ohlcv.csv.gz'\n",
    "POLICY_SAVE_PATH = 'policy'\n",
    "\n",
    "# Model Hyperparameters\n",
    "NUM_TRAINING_STEPS = 20000  # Increased for more meaningful training\n",
    "NUM_TESTING_STEPS = 5000\n",
    "ALPHA = 1.0 # LinUCB exploration parameter\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "TECHNICAL_FEATURES = ['price_change', 'volume_change', 'rsi', 'macd', 'macd_signal', 'macd_hist', 'bb_percent_b', 'bb_bandwidth']\n",
    "TECHNICAL_FEATURES = ['price_change', 'rsi', 'macd_signal', 'macd_hist', 'bb_percent_b', 'bb_bandwidth']\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(DATA_FILEPATH, compression='gzip', parse_dates=['timestamp']) #.set_index('timestamp')\n",
    "all_data = preprocess_data(df) #.iloc[3000:]\n",
    "observation_df, prices_df = create_wide_format_data(\n",
    "    all_data, \n",
    "    symbols=SYMBOLS, \n",
    "    features=TECHNICAL_FEATURES\n",
    ")\n",
    "\n",
    "train_observation_df, test_observation_df = train_test_split(\n",
    "    observation_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_prices_df, test_prices_df = train_test_split(\n",
    "    prices_df, \n",
    "    test_size=TEST_SIZE, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_observation_scaled_df = pd.DataFrame(scaler.fit_transform(train_observation_df), columns=train_observation_df.columns, index=train_observation_df.index)\n",
    "test_observation_scaled_df = pd.DataFrame(scaler.transform(test_observation_df), columns=train_observation_df.columns, index=test_observation_df.index)\n",
    "\n",
    "\n",
    "print(f\"all_data: {all_data.shape}\")\n",
    "print(f\"observation training data: {train_observation_scaled_df.shape}\")\n",
    "print(f\"price training data: {train_prices_df.shape}\")\n",
    "print(f\"observation testing data: {test_observation_scaled_df.shape}\")\n",
    "print(f\"price testing data: {test_prices_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-20 17:51:30.144108: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.151028: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.153942: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.158308: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.161147: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.164016: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.264066: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.265458: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.266697: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-20 17:51:30.267925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5692 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 17848 steps...\n",
      "300/17848 Reward: -0.0024339999072253704"
     ]
    }
   ],
   "source": [
    "# Setup Environment\n",
    "crypto_env = CryptoTradingEnvironment(\n",
    "    observation_df=train_observation_scaled_df,\n",
    "    prices_df=train_prices_df,\n",
    "    symbols=SYMBOLS,\n",
    "    seed_fund = 500.0,\n",
    "    trade_size = 50.0,\n",
    "    trade_fee = 0.002,\n",
    "    invalid_action_penalty = -1.0\n",
    ")\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(crypto_env)\n",
    "\n",
    "# Setup Agent\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec(),\n",
    "    alpha=ALPHA,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# Setup Metrics and Oracle\n",
    "def optimal_reward_oracle(trajectory) -> np.float32:\n",
    "    \"\"\"\n",
    "    Calculates the best possible reward for the current step by looking ahead.\n",
    "    This \"perfect foresight\" oracle is used for calculating regret.\n",
    "\n",
    "    NOTE: The 'trajectory' argument is unused but required by the metric's API.\n",
    "    \"\"\"\n",
    "    # 1. Get the env\n",
    "    py_env = tf_env.pyenv.envs[0]\n",
    "    \n",
    "    # 2. Get the current step index from the environment's state.\n",
    "    current_step = py_env.current_step\n",
    "    price_data = py_env.price_data\n",
    "\n",
    "    \n",
    "    # 3. Handle the edge case where we are at the end of the data.\n",
    "    # We can't look one step into the future.\n",
    "    if trajectory.is_last():\n",
    "        return 0.0\n",
    "\n",
    "    # 4. Calculate the reward for every possible action to find the maximum.\n",
    "    all_possible_rewards = []\n",
    "    num_actions = py_env.action_spec().maximum + 1\n",
    "    \n",
    "    for action in range(num_actions):\n",
    "        # Decode the action into a symbol and a trade type\n",
    "        crypto_index = action // 3\n",
    "        trade_type_idx = action % 3  # 0: BUY, 1: HOLD, 2: SELL\n",
    "        \n",
    "        # The reward for a HOLD action is always 0.\n",
    "        if trade_type_idx == 1:\n",
    "            all_possible_rewards.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        symbol_to_trade = py_env.symbols[crypto_index]\n",
    "        \n",
    "        current_price = price_data.iloc[current_step][symbol_to_trade]\n",
    "        next_price = price_data.iloc[current_step + 1][symbol_to_trade]\n",
    "        \n",
    "        # Calculate the reward for this specific BUY or SELL action\n",
    "        if trade_type_idx == 0:  # BUY\n",
    "            reward = (next_price - current_price) / current_price\n",
    "        else:  # SELL (trade_type_idx == 2)\n",
    "            reward = (current_price - next_price) / current_price\n",
    "        \n",
    "        all_possible_rewards.append(reward)\n",
    "        \n",
    "    # 5. Return the maximum possible reward from all actions.\n",
    "    return np.max(all_possible_rewards).astype(np.float32)\n",
    "\n",
    "class ShowProgress:\n",
    "    def __init__(self, total, interval=50):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "        self.interval = interval\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % self.interval == 0:\n",
    "            print(\"\\r{}/{} Reward: {}\".format(self.counter, self.total, np.round(trajectory.reward.numpy()[0], 6)), end=\"\")\n",
    "\n",
    "class MetricsObserver:\n",
    "    def __init__(self, oracle_fn):\n",
    "        self._oracle_fn = oracle_fn\n",
    "        self.rewards = []\n",
    "        self.regrets = []\n",
    "        self.optimal_rewards = []\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            reward = trajectory.reward.numpy()[0]\n",
    "            optimal_reward = self._oracle_fn(trajectory)\n",
    "            regret = optimal_reward - reward\n",
    "            \n",
    "            self.rewards.append(reward)\n",
    "            self.regrets.append(regret)\n",
    "            self.optimal_rewards.append(optimal_reward)\n",
    "\n",
    "    def cum_rewards(self):\n",
    "        cumulative_reward = np.cumsum(self.rewards)\n",
    "        cumulative_optimal_reward = np.cumsum(self.optimal_rewards)\n",
    "        return cumulative_reward, cumulative_optimal_reward\n",
    "\n",
    "    def cum_averages(self):\n",
    "        steps = np.arange(len(self.rewards)) + 1\n",
    "        cumulative_avg_reward = np.cumsum(self.rewards) / steps\n",
    "        cumulative_avg_optimal_reward = np.cumsum(self.optimal_rewards) / steps\n",
    "        cumulative_avg_regret = np.cumsum(self.regrets) / steps\n",
    "        return cumulative_avg_reward, cumulative_avg_optimal_reward, cumulative_avg_regret\n",
    "        \n",
    "\n",
    "def train_step(trajectory):\n",
    "    if not trajectory.is_last():\n",
    "        time_axised_trajectory = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), trajectory)\n",
    "        agent.train(time_axised_trajectory)\n",
    "        \n",
    "metrics_observer = MetricsObserver(optimal_reward_oracle)\n",
    "\n",
    "# 5. Setup Driver\n",
    "# Set num_steps to the total number of steps you want to train for.\n",
    "num_steps_to_train = min(len(train_observation_scaled_df) - 1, NUM_TRAINING_STEPS)\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=num_steps_to_train,\n",
    "    observers=[train_step, metrics_observer, ShowProgress(num_steps_to_train)]\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting training for {num_steps_to_train} steps...\")\n",
    "driver.run()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save Policy\n",
    "print(f\"\\nSaving the trained policy to: {POLICY_SAVE_PATH}\")\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "saver.save(POLICY_SAVE_PATH)\n",
    "print(\"Policy saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_avg_reward, cumulative_avg_optimal_reward, cumulative_avg_regret = metrics_observer.cum_averages()\n",
    "\n",
    "plt.plot(cumulative_avg_regret)\n",
    "plt.title('Cumulative Average Regret Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_avg_reward, label='Actual')\n",
    "plt.plot(cumulative_avg_optimal_reward, label='Optimal')\n",
    "plt.title('Actual vs. Optimal Cumulative Average Reward Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_reward, cumulative_optimal_reward = metrics_observer.cum_rewards()\n",
    "\n",
    "plt.plot(cumulative_reward, label='Actual')\n",
    "plt.plot(cumulative_optimal_reward, label='Optimal')\n",
    "plt.title('Actual vs. Optimal Cumulative Reward Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_value_history = tf_env.pyenv.envs[0].portfolio_value_history\n",
    "\n",
    "plt.plot(portfolio_value_history)\n",
    "plt.title('Portfolio value Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history = tf_env.pyenv.envs[0].trade_history\n",
    "\n",
    "trade_history_df = pd.DataFrame(trade_history, columns=['Symbol', 'Valid', 'Type', 'Value'])\n",
    "trade_history_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_history_df['Valid'].value_counts().plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_to_test = min(len(test_observation_scaled_df) - 1, NUM_TESTING_STEPS)\n",
    "\n",
    "eval_env = CryptoTradingEnvironment(\n",
    "    observation_df=test_observation_scaled_df[:num_steps_to_test],\n",
    "    prices_df=test_prices_df[:num_steps_to_test],\n",
    "    symbols=SYMBOLS,\n",
    "    seed_fund = 500.0,\n",
    "    trade_size = 50.0,\n",
    "    trade_fee = 0.002,\n",
    "    invalid_action_penalty = -1.0\n",
    ")\n",
    "\n",
    "# 4. Run the Evaluation Loop\n",
    "print(f\"\\n--- Starting Evaluation on Test Set ({len(test_observation_scaled_df)} steps) ---\")\n",
    "\n",
    "time_step = eval_env.reset()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Loop until the environment signals the episode is over\n",
    "while not time_step.is_last():\n",
    "    # The policy expects a batch dimension. We create a new TimeStep\n",
    "    # object with the observation correctly batched.\n",
    "    counter += 1\n",
    "    if counter % 50 == 0:\n",
    "        print(f\"\\rTraining step {counter}/{num_steps_to_test}\", end=\"\")\n",
    "    \n",
    "    # A. Add the batch dimension to the observation tensor.\n",
    "    observation_batch = tf.expand_dims(time_step.observation, 0)\n",
    "    \n",
    "    # B. Use the ._replace() method to create a new TimeStep with the batched observation.\n",
    "    # This is the correct way to modify a TimeStep.\n",
    "    batched_time_step = time_step._replace(observation=observation_batch)\n",
    "    \n",
    "    # C. Get an action from the loaded policy using the batched TimeStep.\n",
    "    action_step = agent.policy.action(batched_time_step)\n",
    "    action = action_step.action.numpy()[0]\n",
    "    \n",
    "    # D. Step the environment with the original, unbatched action.\n",
    "    time_step = eval_env.step(action)\n",
    "\n",
    "print(\"\\n--- Evaluation Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Report and Plot the Results\n",
    "balance_history = eval_env.balance_history\n",
    "asset_value_history = eval_env.asset_value_history\n",
    "portfolio_value_history = eval_env.portfolio_value_history\n",
    "seed_fund_value = balance_history[0]\n",
    "final_portfolio_value = portfolio_value_history[-1]\n",
    "total_return_pct = ((final_portfolio_value - seed_fund_value) / seed_fund_value) * 100\n",
    "\n",
    "print(f\"Initial Portfolio Value: ${seed_fund_value:,.2f}\")\n",
    "print(f\"Final Portfolio Value:   ${final_portfolio_value:,.2f}\")\n",
    "print(f\"Total Return on Test Set: {total_return_pct:.2f}%\")\n",
    "\n",
    "# Plot the portfolio value over time\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.stackplot(test_prices_df.index[:len(balance_history)],[balance_history, asset_value_history], labels=['Cash Balance','Asset Value'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Agent Performance on Hold-Out Test Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio Value ($)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
