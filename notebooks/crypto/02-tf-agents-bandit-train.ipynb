{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:34:09.900150: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-14 17:34:09.927362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-14 17:34:09.927381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-14 17:34:09.928061: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-14 17:34:09.932298: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Bandit Training Script ---\n",
      "Loading data from data/ohlcv.csv.gz...\n",
      "Processing symbols: ['BTC', 'DOGE', 'XRP', 'ETH', 'SOL']\n",
      "Data prepared. Shape: (13093, 15)\n",
      "\n",
      "Starting training for 985 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 17:34:11.550478: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-07-14 17:34:11.550495: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: d7a4e586fbec\n",
      "2025-07-14 17:34:11.550498: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: d7a4e586fbec\n",
      "2025-07-14 17:34:11.550541: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 570.158.1\n",
      "2025-07-14 17:34:11.550549: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 570.158.1\n",
      "2025-07-14 17:34:11.550552: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 570.158.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/985 Reward: [0.00427779]]Training finished.\n",
      "\n",
      "Saving the trained policy to: policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n",
      "WARNING:absl:`0/reward` is not a valid tf.function parameter name. Sanitizing to `arg_0_reward`.\n",
      "WARNING:absl:`0/discount` is not a valid tf.function parameter name. Sanitizing to `arg_0_discount`.\n",
      "WARNING:absl:`0/observation` is not a valid tf.function parameter name. Sanitizing to `arg_0_observation`.\n",
      "WARNING:absl:`0/step_type` is not a valid tf.function parameter name. Sanitizing to `arg_0_step_type`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: policy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tfp.distributions.Deterministic_ACTTypeSpec; loading this StructuredValue will require that this type be imported and registered.\n",
      "  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\n",
      "INFO:tensorflow:Assets written to: policy/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy saved successfully.\n",
      "\n",
      "Cumulative Regret vs. Perfect Foresight Oracle: 0.0063\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "from utils import load_and_prepare_data, SYMBOLS, DATA_FILEPATH, POLICY_SAVE_PATH, CONTEXT_LENGTH, NUM_TRAINING_STEPS, ALPHA\n",
    "from environment import CryptoTradingEnvironment\n",
    "\n",
    "# --- Main Training Script ---\n",
    "print(\"--- Starting Bandit Training Script ---\")\n",
    "\n",
    "# 1. Load Data\n",
    "all_data = load_and_prepare_data(DATA_FILEPATH, SYMBOLS)\n",
    "training_data = all_data.iloc[:NUM_TRAINING_STEPS]\n",
    "\n",
    "# 2. Setup Environment\n",
    "tf_env = tf_py_environment.TFPyEnvironment(\n",
    "    CryptoTradingEnvironment(data=training_data, symbols=SYMBOLS, context_len=CONTEXT_LENGTH)\n",
    ")\n",
    "\n",
    "# 3. Setup Agent\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec(),\n",
    "    alpha=ALPHA,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# 4. Setup Metrics and Oracle\n",
    "def optimal_reward_oracle(observation):\n",
    "    py_env = tf_env.pyenv.envs[0]\n",
    "    current_step = py_env._current_step_index\n",
    "    if current_step >= len(py_env._data) - 2: return 0.0\n",
    "    \n",
    "    rewards = []\n",
    "    for action in range(py_env.action_spec().maximum + 1):\n",
    "        idx, is_buy = action // 2, action % 2 == 0\n",
    "        col = f'{py_env._symbols[idx]}_close'\n",
    "        p_curr = py_env._data.iloc[current_step][col]\n",
    "        p_next = py_env._data.iloc[current_step + 1][col]\n",
    "        reward = (p_next - p_curr) / p_curr if is_buy else (p_curr - p_next) / p_curr\n",
    "        rewards.append(reward)\n",
    "    return np.max(rewards).astype(np.float32)\n",
    "\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_oracle)\n",
    "\n",
    "class ShowProgress:\n",
    "    def __init__(self, total, interval=50):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % interval == 0:\n",
    "            print(\"\\r{}/{} Reward: {}\".format(self.counter, self.total, trajectory.reward), end=\"\")\n",
    "\n",
    "# 5. Setup Driver\n",
    "def train_step(trajectory):\n",
    "    if not trajectory.is_last():\n",
    "        time_axised_trajectory = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), trajectory)\n",
    "        agent.train(time_axised_trajectory)\n",
    "        \n",
    "num_steps = len(training_data) - CONTEXT_LENGTH - 5\n",
    "\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=num_steps,\n",
    "    observers=[train_step, regret_metric, ShowProgress(num_steps)]\n",
    ")\n",
    "\n",
    "# 6. Run Training\n",
    "print(f\"\\nStarting training for {driver._num_steps} steps...\")\n",
    "driver.run()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 7. Save Policy\n",
    "print(f\"\\nSaving the trained policy to: {POLICY_SAVE_PATH}\")\n",
    "saver = policy_saver.PolicySaver(agent.policy)\n",
    "saver.save(POLICY_SAVE_PATH)\n",
    "print(\"Policy saved successfully.\")\n",
    "\n",
    "# 8. Report Results\n",
    "cumulative_regret = regret_metric.result().numpy()\n",
    "print(f\"\\nCumulative Regret vs. Perfect Foresight Oracle: {cumulative_regret:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.axhline(y=0.0, color='r', linestyle='-')\n",
    "# plt.plot(rewards)\n",
    "# plt.ylabel('Rewards')\n",
    "# plt.xlabel('Number of Iterations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
