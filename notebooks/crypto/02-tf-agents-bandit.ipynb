{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"gpus: {gpus}\")\n",
    "\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import bandit_py_environment\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "# --- 1. Data Simulation ---\n",
    "def generate_dummy_data(num_steps, num_cryptos):\n",
    "    \"\"\"Generates a DataFrame with dummy crypto prices.\"\"\"\n",
    "    data = {}\n",
    "    initial_prices = np.array([40000, 2000, 1.5])\n",
    "    for i, name in enumerate(['BTC', 'ETH', 'ADA']):\n",
    "        prices = np.zeros(num_steps)\n",
    "        prices[0] = initial_prices[i]\n",
    "        for t in range(1, num_steps):\n",
    "            prices[t] = prices[t-1] * (1 + np.random.normal(0.0, 0.02))\n",
    "        data[f'{name}_close'] = prices\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "NUM_CRYPTOS = 3\n",
    "CRYPTO_NAMES = ['BTC', 'ETH', 'ADA']\n",
    "NUM_STEPS = 2000\n",
    "data = generate_dummy_data(NUM_STEPS, NUM_CRYPTOS)\n",
    "\n",
    "# --- 2. Create the Bandit Environment ---\n",
    "class CryptoTradingEnvironment(bandit_py_environment.BanditPyEnvironment):\n",
    "    def __init__(self, data, lookback_window=10):\n",
    "        self._data = data\n",
    "        self._lookback = lookback_window\n",
    "        self._num_cryptos = len(CRYPTO_NAMES)\n",
    "        self._num_actions = self._num_cryptos * 2\n",
    "        self._current_step = self._lookback\n",
    "        \n",
    "        observation_size = self._num_cryptos * (self._lookback - 1)\n",
    "        observation_spec = array_spec.ArraySpec(shape=(observation_size,), dtype=np.float32, name='context')\n",
    "        action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=self._num_actions - 1, name='action')\n",
    "        \n",
    "        super(CryptoTradingEnvironment, self).__init__(observation_spec, action_spec)\n",
    "\n",
    "    def _observe(self):\n",
    "        start_idx_price = self._current_step - self._lookback\n",
    "        end_idx_price = self._current_step\n",
    "        price_window = self._data.iloc[start_idx_price:end_idx_price]\n",
    "        returns = price_window.pct_change().dropna()\n",
    "        flat_returns = returns[[f'{name}_close' for name in CRYPTO_NAMES]].values.flatten()\n",
    "        required_size = self._num_cryptos * (self._lookback - 1)\n",
    "        padding_size = required_size - len(flat_returns)\n",
    "        if padding_size > 0:\n",
    "            flat_returns = np.pad(flat_returns, (0, padding_size), 'constant')\n",
    "        return flat_returns.astype(np.float32)\n",
    "\n",
    "    def _apply_action(self, action):\n",
    "        if self._current_step >= len(self._data) - 1:\n",
    "            self._current_step += 1\n",
    "            return 0.0\n",
    "        crypto_index = action // 2\n",
    "        is_buy_action = action % 2 == 0\n",
    "        current_price = self._data.iloc[self._current_step][f'{CRYPTO_NAMES[crypto_index]}_close']\n",
    "        next_price = self._data.iloc[self._current_step + 1][f'{CRYPTO_NAMES[crypto_index]}_close']\n",
    "        reward = ((next_price - current_price) / current_price) if is_buy_action else ((current_price - next_price) / current_price)\n",
    "        self._current_step += 1\n",
    "        return reward\n",
    "\n",
    "# --- 3. Instantiate Environment and Agent ---\n",
    "LOOKBACK_WINDOW = 10\n",
    "tf_env = tf_py_environment.TFPyEnvironment(CryptoTradingEnvironment(data, lookback_window=LOOKBACK_WINDOW))\n",
    "\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=ts.time_step_spec(tf_env.observation_spec()),\n",
    "    action_spec=tf_env.action_spec(),\n",
    "    alpha=1.0, # Exploration parameter\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# --- 4. Define the Training Loop ---\n",
    "def train_step(trajectory):\n",
    "    time_axised_trajectory = tf.nest.map_structure(lambda t: tf.expand_dims(t, 1), trajectory)\n",
    "    return agent.train(time_axised_trajectory)\n",
    "\n",
    "# Simple observer to collect rewards during training\n",
    "training_rewards = []\n",
    "def collect_reward_observer(trajectory):\n",
    "  training_rewards.append(trajectory.reward.numpy()[0])\n",
    "\n",
    "NUM_TRAINING_STEPS = 1000\n",
    "\n",
    "def get_action_name(action):\n",
    "    crypto_index = action // 2\n",
    "    action_type = \"BUY\" if action % 2 == 0 else \"SELL\"\n",
    "    action_name = f\"{action_type} {CRYPTO_NAMES[crypto_index]}\"\n",
    "    print(action_name)\n",
    "    return action_name\n",
    "    \n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=NUM_TRAINING_STEPS,\n",
    "    observers=[train_step, collect_reward_observer]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "driver.run()\n",
    "print(\"Training finished.\")\n",
    "print(f\"Total reward earned during training: {sum(training_rewards)}\")\n",
    "print(f\"Average reward per step: {np.mean(training_rewards):.4f}\")\n",
    "\n",
    "# --- 5. Evaluate the Results ---\n",
    "print(\"\\n--- Evaluation Loop ---\")\n",
    "time_step = tf_env.reset() # Reset env to start from a new position for eval\n",
    "cumulative_reward = 0\n",
    "for i in range(50):\n",
    "    action_step = agent.policy.action(time_step)\n",
    "    action = action_step.action.numpy()[0]\n",
    "    time_step = tf_env.step(action)\n",
    "    reward = time_step.reward.numpy()[0]\n",
    "    cumulative_reward += reward\n",
    "    print(f\"Step {i+1}: Chose action '{get_action_name(action)}', received reward {reward:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal cumulative reward over last 50 steps: {cumulative_reward:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
