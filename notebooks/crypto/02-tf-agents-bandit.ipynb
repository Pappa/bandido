{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents import lin_ucb_agent\n",
    "from tf_agents.bandits.environments import bandit_py_environment\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Simulation (Replace with your real data) ---\n",
    "# For this example, we simulate data. In a real scenario, you would load\n",
    "# your OHLCV data here.\n",
    "def generate_dummy_data(num_steps, num_cryptos):\n",
    "    \"\"\"Generates a DataFrame with dummy crypto prices.\"\"\"\n",
    "    data = {}\n",
    "    # Use a random walk for slightly more realistic price movement\n",
    "    initial_prices = np.array([40000, 2000, 1.5])\n",
    "    for i, name in enumerate(['BTC', 'ETH', 'ADA']):\n",
    "        prices = np.zeros(num_steps)\n",
    "        prices[0] = initial_prices[i]\n",
    "        for t in range(1, num_steps):\n",
    "            prices[t] = prices[t-1] * (1 + np.random.normal(0.0, 0.02)) # 2% std dev\n",
    "        data[f'{name}_close'] = prices\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "NUM_CRYPTOS = 3\n",
    "CRYPTO_NAMES = ['BTC', 'ETH', 'ADA']\n",
    "NUM_STEPS = 2000\n",
    "data = generate_dummy_data(NUM_STEPS, NUM_CRYPTOS)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Create the Bandit Environment ---\n",
    "class CryptoTradingEnvironment(bandit_py_environment.BanditPyEnvironment):\n",
    "    def __init__(self, data, lookback_window=10):\n",
    "        self._data = data\n",
    "        self._lookback = lookback_window\n",
    "        self._num_cryptos = len(CRYPTO_NAMES)\n",
    "        self._num_actions = self._num_cryptos * 2  # Buy/Sell for each crypto\n",
    "        self._current_step = self._lookback  # Start after the first lookback period\n",
    "        \n",
    "        # Define observation and action specs\n",
    "        # Observation: past `lookback_window` returns for each crypto\n",
    "        observation_spec = array_spec.ArraySpec(\n",
    "            shape=(self._num_cryptos * self._lookback,),\n",
    "            dtype=np.float32,\n",
    "            name='context'\n",
    "        )\n",
    "        # Action: Buy/Sell for each crypto\n",
    "        action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=self._num_actions - 1, name='action'\n",
    "        )\n",
    "        \n",
    "        super(CryptoTradingEnvironment, self).__init__(observation_spec, action_spec)\n",
    "\n",
    "    def _observe(self):\n",
    "        \"\"\"Generate the context vector.\"\"\"\n",
    "        start = self._current_step - self._lookback\n",
    "        end = self._current_step\n",
    "        \n",
    "        # Calculate pct returns for the lookback window\n",
    "        returns = self._data.iloc[start:end].pct_change().dropna()\n",
    "        \n",
    "        # Flatten the returns into a single context vector\n",
    "        # Pad with zeros if there are not enough returns (at the beginning)\n",
    "        flat_returns = returns[[f'{name}_close' for name in CRYPTO_NAMES]].values.flatten()\n",
    "        \n",
    "        # Ensure consistent size\n",
    "        required_size = self._num_cryptos * (self._lookback - 1)\n",
    "        padding_size = required_size - len(flat_returns)\n",
    "        if padding_size > 0:\n",
    "            flat_returns = np.pad(flat_returns, (0, padding_size), 'constant')\n",
    "\n",
    "        return flat_returns.astype(np.float32)\n",
    "\n",
    "    def _apply_action(self, action):\n",
    "        \"\"\"Calculate the reward for the chosen action.\"\"\"\n",
    "        crypto_index = action // 2\n",
    "        is_buy_action = action % 2 == 0\n",
    "\n",
    "        current_price = self._data.iloc[self._current_step][f'{CRYPTO_NAMES[crypto_index]}_close']\n",
    "        next_price = self._data.iloc[self._current_step + 1][f'{CRYPTO_NAMES[crypto_index]}_close']\n",
    "        \n",
    "        # Simple reward: percent change\n",
    "        if is_buy_action:\n",
    "            # Reward for buying is positive if price goes up\n",
    "            reward = (next_price - current_price) / current_price\n",
    "        else: # Sell action\n",
    "            # Reward for selling is positive if price goes down\n",
    "            reward = (current_price - next_price) / current_price\n",
    "\n",
    "        self._current_step += 1\n",
    "        return reward\n",
    "\n",
    "# --- 3. Instantiate Environment and Agent ---\n",
    "LOOKBACK_WINDOW = 10\n",
    "NUM_ACTIONS = len(CRYPTO_NAMES) * 2\n",
    "\n",
    "# Wrap the Python environment in a TF environment\n",
    "tf_env = tf_py_environment.TFPyEnvironment(\n",
    "    CryptoTradingEnvironment(data, lookback_window=LOOKBACK_WINDOW)\n",
    ")\n",
    "\n",
    "# Create the LinUCB Agent\n",
    "observation_spec = tf_env.observation_spec()\n",
    "time_step_spec = ts.time_step_spec(observation_spec)\n",
    "action_spec = tf_env.action_spec()\n",
    "\n",
    "agent = lin_ucb_agent.LinearUCBAgent(\n",
    "    time_step_spec=time_step_spec,\n",
    "    action_spec=action_spec,\n",
    "    alpha=1.0,  # Alpha controls exploration. Higher alpha = more exploration.\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "# --- 4. Define the Training Loop ---\n",
    "NUM_TRAINING_STEPS = 1000 # Must be less than NUM_STEPS - LOOKBACK_WINDOW\n",
    "regret_metric = tf_bandit_metrics.RegretMetric(lambda: 0) # Simple optimal reward is 0\n",
    "\n",
    "def get_action_name(action):\n",
    "    crypto_index = action // 2\n",
    "    action_type = \"BUY\" if action % 2 == 0 else \"SELL\"\n",
    "    return f\"{action_type} {CRYPTO_NAMES[crypto_index]}\"\n",
    "    \n",
    "# Use a driver to run the loop\n",
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=tf_env,\n",
    "    policy=agent.policy,\n",
    "    num_steps=NUM_TRAINING_STEPS,\n",
    "    observers=[regret_metric] # You can add more observers, like saving trajectories\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "driver.run()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 5. Evaluate the Results ---\n",
    "cumulative_reward = regret_metric.result()\n",
    "print(f\"Total reward earned over {NUM_TRAINING_STEPS} steps: {cumulative_reward.numpy()}\")\n",
    "\n",
    "# Let's check the last few decisions\n",
    "time_step = tf_env.reset()\n",
    "cumulative_reward = 0\n",
    "for i in range(50):\n",
    "    action_step = agent.policy.action(time_step)\n",
    "    action = action_step.action.numpy()[0]\n",
    "    time_step = tf_env.step(action)\n",
    "    reward = time_step.reward.numpy()[0]\n",
    "    cumulative_reward += reward\n",
    "    print(f\"Step {i}: Chose action '{get_action_name(action)}', received reward {reward:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal cumulative reward over last 50 steps: {cumulative_reward:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
