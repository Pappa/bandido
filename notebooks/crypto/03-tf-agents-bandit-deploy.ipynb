{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# We only need a few imports for deployment\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "\n",
    "from utils import load_and_prepare_data, SYMBOLS\n",
    "\n",
    "POLICY_SAVE_PATH = 'policy'\n",
    "CRYPTO_NAMES = SYMBOLS\n",
    "CONTEXT_LENGTH = 10\n",
    "NUM_TRAINING_STEPS = 1000\n",
    "\n",
    "\n",
    "# --- Helper functions to simulate the live environment ---\n",
    "def get_action_name(action):\n",
    "    crypto_index = action // 2\n",
    "    action_type = \"BUY\" if action % 2 == 0 else \"SELL\"\n",
    "    return f\"{action_type} {CRYPTO_NAMES[crypto_index]}\"\n",
    "\n",
    "def get_live_minimal_observation(row):\n",
    "    \"\"\"Creates a minimal observation from a single row of live data.\"\"\"\n",
    "    obs_slice = []\n",
    "    for symbol in CRYPTO_NAMES:\n",
    "        obs_slice.append(row[f'{symbol}_close_return'])\n",
    "        obs_slice.append(row[f'{symbol}_volume_return'])\n",
    "    return np.array(obs_slice, dtype=np.float32)\n",
    "\n",
    "def create_context_from_buffer(buffer):\n",
    "    \"\"\"Samples from the buffer to create the context vector for the policy.\"\"\"\n",
    "    num_items = buffer.num_frames()\n",
    "    dataset = buffer.as_dataset(single_deterministic_pass=True)\n",
    "    batched_items = next(iter(dataset.batch(num_items)))\n",
    "    \n",
    "    # Select only the data tensor (at index 0) from the (data, info) tuple\n",
    "    all_items = batched_items[0]\n",
    "\n",
    "    # The squeeze operation is removed from here as well.\n",
    "    context = tf.reshape(all_items[-CONTEXT_LENGTH:], [-1])\n",
    "    return tf.expand_dims(context, 0)\n",
    "\n",
    "# --- Main Deployment Logic ---\n",
    "print(f\"Loading trained policy from {POLICY_SAVE_PATH}...\")\n",
    "# 1. Load the policy from the saved directory\n",
    "loaded_policy = tf.saved_model.load(POLICY_SAVE_PATH)\n",
    "\n",
    "# --- Simulate a live data feed ---\n",
    "# In a real system, this data would come from a WebSocket or REST API call.\n",
    "# We'll just load our data file again for simulation.\n",
    "# Let's pretend the first part of the data was for training, and the next part is live.\n",
    "all_data = load_and_prepare_data('data/ohlcv.csv.gz', CRYPTO_NAMES)\n",
    "live_data_stream = all_data.iloc[NUM_TRAINING_STEPS:]\n",
    "\n",
    "print(f\"\\n--- Starting Live Inference Simulation ({len(live_data_stream)} steps) ---\")\n",
    "\n",
    "# Setup a buffer to maintain the state, just like in the environment\n",
    "num_features = len(CRYPTO_NAMES) * 2\n",
    "data_spec = tensor_spec.TensorSpec([num_features], dtype=tf.float32, name='minimal_observation')\n",
    "live_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=data_spec, batch_size=1, max_length=CONTEXT_LENGTH + 5\n",
    ")\n",
    "\n",
    "# Pre-fill the buffer with initial data\n",
    "for i in range(CONTEXT_LENGTH):\n",
    "    minimal_obs = get_live_minimal_observation(live_data_stream.iloc[i])\n",
    "    live_buffer.add_batch(tf.expand_dims(minimal_obs, 0))\n",
    "\n",
    "# The main loop of the trading bot\n",
    "for i in range(CONTEXT_LENGTH, len(live_data_stream)):\n",
    "    # 1. Get current context from our state buffer\n",
    "    current_context = create_context_from_buffer(live_buffer)\n",
    "    \n",
    "    # 2. Construct a TimeStep object for the policy\n",
    "    # For a continuous stream, we use a transition. The reward can be a dummy value.\n",
    "    time_step = ts.transition(observation=current_context, reward=tf.constant([0.0]))\n",
    "    \n",
    "    # 3. GET THE ACTION FROM THE POLICY\n",
    "    action_step = loaded_policy.action(time_step)\n",
    "    action = action_step.action.numpy()[0]\n",
    "    \n",
    "    print(f\"Step {i}: Context ready. Policy chose action: {get_action_name(action)}\")\n",
    "    # In a real bot: Execute trade via API based on `action`\n",
    "    \n",
    "    # 4. Update the buffer with the latest data point to prepare for the next step\n",
    "    latest_minimal_obs = get_live_minimal_observation(live_data_stream.iloc[i])\n",
    "    live_buffer.add_batch(tf.expand_dims(latest_minimal_obs, 0))\n",
    "    \n",
    "    time.sleep(0.1) # Simulate waiting for the next data candle\n",
    "\n",
    "print(\"\\nLive simulation finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
